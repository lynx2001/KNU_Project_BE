"""
qa_agent.py â€” LLM ê¸°ë°˜ ë¼ìš°íŒ… + Smalltalk / Internal RAG / WebSearch 3-ëª¨ë“œ QA Agent
ê·¸ë˜í”„ ë…¸ë“œì—ì„œ ë°”ë¡œ í˜¸ì¶œ ê°€ëŠ¥í•œ ë²„ì „.
"""

from __future__ import annotations
import os, re
from typing import Optional, Dict, Any, List, Tuple, Literal
from dataclasses import dataclass
from dotenv import load_dotenv
from pydantic import BaseModel

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import AIMessage
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

load_dotenv()

# ============================================================
# ğŸ”§ ë””ë²„ê·¸ ìŠ¤ìœ„ì¹˜
# ============================================================
DEBUG = True
def dprint(*args, **kwargs):
    if DEBUG:
        print("[DBG qa]", *args, **kwargs)


# ============================================================
# ğŸ“˜ ë‚´ë¶€ ìš”ì•½ ê¸°ë°˜ RAG ìœ í‹¸
# ============================================================
ORDINAL_PAT = re.compile(r"(ì²« ?ë²ˆì§¸|ë‘ ?ë²ˆì§¸|ì„¸ ?ë²ˆì§¸|1 ?ë²ˆ|2 ?ë²ˆ|3 ?ë²ˆ)")
SUMMARY_HINT_PAT = re.compile(r"(ìš”ì•½|ì˜¤ëŠ˜|ì •ë¦¬|í•µì‹¬)", re.IGNORECASE)

def _resolve_ordinal_korean(text: str) -> Optional[int]:
    t = text.replace(" ", "")
    if "ì²«ë²ˆì§¸" in t or "1ë²ˆ" in t: return 0
    if "ë‘ë²ˆì§¸" in t or "2ë²ˆ" in t: return 1
    if "ì„¸ë²ˆì§¸" in t or "3ë²ˆ" in t: return 2
    return None

def _collect_internal_corpus(state: Dict[str, Any]) -> List[Tuple[str, str]]:
    corpus: List[Tuple[str, str]] = []
    ctx = state.get("context", {})
    sums = ctx.get("summaries") or []
    arts = ctx.get("selected_articles") or []

    for i, s in enumerate(sums):
        body = ""
        if s.get("tl_dr"): body += s["tl_dr"] + "\n"
        if s.get("bullets"):
            body += "\n".join(f"- {b}" for b in s["bullets"])
        if body.strip():
            corpus.append((f"summary:{i}", body))
    for i, a in enumerate(arts):
        content = (a.get("content") or "").strip()
        if content:
            corpus.append((f"article:{i}", content[:4000]))
    return corpus

def _build_ephemeral_store(corpus: List[Tuple[str, str]]) -> Optional[FAISS]:
    if not corpus:
        return None
    docs = [Document(page_content=txt, metadata={"doc_id": did}) for did, txt in corpus]
    splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=80)
    splits = splitter.split_documents(docs)
    vs = FAISS.from_documents(splits, OpenAIEmbeddings())
    return vs

def _internal_rag_answer(
    question: str,
    state: Dict[str, Any],
    level: str = "beginner",
    top_k: int = 4,
    force_pick: Optional[int] = None
) -> Optional[AIMessage]:
    corpus = _collect_internal_corpus(state)
    if not corpus:
        dprint("internal RAG: no corpus â†’ fallback")
        return None

    selected_texts: List[str] = []
    if force_pick is not None:
        for did, txt in corpus:
            if did == f"summary:{force_pick}" or did == f"article:{force_pick}":
                selected_texts.append(txt)
        if not selected_texts:
            dprint("internal RAG: forced pick not found â†’ search all")

    if not selected_texts:
        vs = _build_ephemeral_store(corpus)
        if vs:
            hits = vs.similarity_search(question, k=top_k)
            selected_texts = [h.page_content for h in hits]
        else:
            selected_texts = []

    ctx_text = "\n\n---\n\n".join(selected_texts[:3]) if selected_texts else "(ê´€ë ¨ ë‚´ë¶€ ìš”ì•½ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"
    sys = (
        "ë„ˆëŠ” ì‚¬ìš©ìê°€ ì˜¤ëŠ˜ í•™ìŠµí•œ ìš”ì•½/ê¸°ì‚¬ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì„¤ëª…í•˜ëŠ” íŠœí„°ì•¼. "
        "ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œë§Œ ë‹µí•˜ê³ , ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ì•„ë¼. "
        f"ì‚¬ìš©ì ìˆ˜ì¤€(level={level})ì— ë§ì¶° ê°„ë‹¨íˆ ì„¤ëª…í•˜ê³ , í•„ìš”í•˜ë©´ í•œ ì¤„ ì˜ˆì‹œë¥¼ ë“¤ì–´ë¼."
    )
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    res = llm.invoke([
        {"role": "system", "content": sys},
        {"role": "user", "content": f"ì§ˆë¬¸: {question}\n\n[ë‚´ë¶€ ì»¨í…ìŠ¤íŠ¸]\n{ctx_text}"},
    ])
    return AIMessage(content=res.content)


# ============================================================
# ğŸ’¬ SMALLTALK
# ============================================================
_SMALLTALK_PAT = re.compile(r"^\s*(ì•ˆë…•|í•˜ì´|í—¬ë¡œ|hello|ë°˜ê°€ì›Œ|ê³ ë§ˆì›Œ|ê°ì‚¬|ì˜\s*ì§€ë‚´|ã…ã…‡)\b", re.IGNORECASE)
def qa_smalltalk(user_text: str) -> AIMessage:
    dprint("mode=SMALLTALK")
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
    res = llm.invoke([
        {"role": "system", "content": "ë„ˆëŠ” ê³µì†í•˜ê³  ê°„ê²°í•˜ê²Œ ëŒ€í™”í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤."},
        {"role": "user", "content": user_text},
    ])
    return AIMessage(content=res.content)


# ============================================================
# ğŸŒ Tavily Search (ê³µì‹ > ì»¤ë®¤ë‹ˆí‹° í´ë°±)
# ============================================================
def _tavily_results(query: str, k: int = 1) -> List[Dict[str, Any]]:
    """Tavily community toolë§Œ ì‚¬ìš© (ê²°ê³¼ ê¸°ë³¸ 1ê°œ)."""
    dprint("WEB.search (community only):", query, "k=", k)
    try:
        from langchain_community.tools.tavily_search import TavilySearchResults
        tool = TavilySearchResults(max_results=k)
        results = tool.invoke({"query": query})
        dprint("tavily community ok; n_results=", len(results) if isinstance(results, list) else "n/a")
        return results if isinstance(results, list) else []
    except Exception as e:
        dprint("tavily community failed:", repr(e))
        return []

def qa_web_summarize(query: str, results: List[Dict[str, Any]], level: str = "beginner") -> AIMessage:
    dprint("mode=WEB.summarize: n_results=", len(results))
    top = results[:1]  # â† ì—¬ê¸°!
    refs_text = "\n".join(
        f"- {r.get('title','(ì œëª©ì—†ìŒ)')} {r.get('url','')}"
        for r in top if isinstance(r, dict)
    ) or "(ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤)"
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    sys = (
        "ë„ˆëŠ” ë‰´ìŠ¤/ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì§ˆì˜ì— ë§ì¶° í•µì‹¬ë§Œ ì •ë¦¬í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. "
        f"ì‚¬ìš©ì ìˆ˜ì¤€(level={level})ì— ë§ì¶° ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ê³ , ê°€ëŠ¥í•œ ê²½ìš° ì°¸ê³ ë§í¬ë„ í•¨ê»˜ ì œê³µí•´."
    )
    res = llm.invoke([
        {"role": "system", "content": sys},
        {"role": "user", "content": f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\nê²€ìƒ‰ ê²°ê³¼(ìƒìœ„ 3ê°œ):\n{refs_text}"},
    ])
    return AIMessage(content=res.content)


# ============================================================
# ğŸ¤– LLM ê¸°ë°˜ ë¼ìš°íŒ…
# ============================================================
class QARouteDecision(BaseModel):
    mode: Literal["smalltalk", "internal", "web"]
    forced_index: Optional[int] = None
    reason: Optional[str] = None

_QA_ROUTE_SYSTEM = (
    "ë„ˆëŠ” QA ì„œë¸Œì—ì´ì „íŠ¸ì˜ ë¼ìš°í„°ì•¼. ì•„ë˜ ì„¸ ëª¨ë“œ ì¤‘ í•˜ë‚˜ë¥¼ ê³ ë¥¸ë‹¤.\n"
    " - smalltalk: ì¸ì‚¬/ì¡ë‹´/ê°ì‚¬ ë“± ë„êµ¬ ë¶ˆí•„ìš”í•œ ì¼ìƒ ëŒ€í™”.\n"
    " - internal: ì‚¬ìš©ìê°€ ì˜¤ëŠ˜ í•™ìŠµí•œ ìš”ì•½/ê¸°ì‚¬ì— ê¸°ëŒ€ì–´ ë‹µí•´ì•¼ í•  ë•Œ. "
    "   'ìš”ì•½', 'ì˜¤ëŠ˜', 'ì²«ë²ˆì§¸/ë‘ë²ˆì§¸/ì„¸ë²ˆì§¸/1ë²ˆ/2ë²ˆ/3ë²ˆ' ê°™ì€ í‘œí˜„ì´ ìˆê³ , "
    "   ë‚´ë¶€ ìš”ì•½(summaries)ì´ ì‹¤ì œë¡œ ì¡´ì¬í•  ë•Œ internalì„ ì„ íƒí•œë‹¤.\n"
    " - web: ê·¸ ë°–ì˜ ëª¨ë“  ì •ë³´ íƒìƒ‰/ì‚¬ì‹¤ í™•ì¸(ì™¸ë¶€ ê²€ìƒ‰ í•„ìš”) ìƒí™©.\n\n"
    "ê·œì¹™:\n"
    "1) ì¸ì‚¬/ì¡ë‹´ì´ë©´ smalltalk.\n"
    "2) ë‚´ë¶€ ìš”ì•½ì„ ì°¸ì¡°í•œ ì§ˆë¬¸ + ë‚´ë¶€ ìš”ì•½ì´ stateì— ì¡´ì¬í•˜ë©´ internal.\n"
    "3) ê·¸ ì™¸ëŠ” web.\n"
    "4) ì‚¬ìš©ìê°€ 'ë‘ë²ˆì§¸/3ë²ˆ' ë“±ì„ ë§í•˜ë©´ forced_indexë¥¼ 0ë¶€í„° ì‹œì‘í•´ ì§€ì •(ë‘ë²ˆì§¸=1, 3ë²ˆ=2). ëª»ì°¾ìœ¼ë©´ null.\n"
    "ë°˜ë“œì‹œ JSONìœ¼ë¡œë§Œ ë‹µí•˜ë¼."
)

def qa_llm_route(user_text: str, has_summaries: bool) -> QARouteDecision:
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0).with_structured_output(QARouteDecision)
    sys = _QA_ROUTE_SYSTEM + f"\n\n[ì»¨í…ìŠ¤íŠ¸] has_summaries={has_summaries}"
    out = llm.invoke([
        {"role": "system", "content": sys},
        {"role": "user", "content": user_text or ""},
    ])
    dprint("llm-route:", out.model_dump())
    return out


# ============================================================
# ğŸ§© Main Entrypoint (ê·¸ë˜í”„ í˜¸ì¶œìš©)
# ============================================================
def handle(user_text: str, profile: Optional[Dict[str, Any]] = None, state: Optional[Dict[str, Any]] = None) -> AIMessage:
    """
    - smalltalk â†’ LLM
    - internal â†’ ë‚´ë¶€ summaries/articles ê¸°ë°˜ RAGë¡œ ë‹µë³€
    - web â†’ Tavily ê²€ìƒ‰ â†’ ìš”ì•½
    """
    try:
        ctx = (state or {}).get("context", {})
        has_summaries = bool(ctx.get("summaries"))
        decision = qa_llm_route(user_text, has_summaries)
        mode = decision.mode
        forced = decision.forced_index
        dprint(f"route decided: mode={mode}, forced_index={forced}, reason={decision.reason}")

        if mode == "smalltalk":
            return qa_smalltalk(user_text)

        if mode == "internal":
            level = (profile or {}).get("level", "beginner")
            ans = _internal_rag_answer(user_text, state or {}, level=level, force_pick=forced)
            if ans is not None:
                return ans
            dprint("internal RAG unavailable â†’ fallback to WEB")

        # WEB (default)
        results = _tavily_results(user_text, k=1)
        level = (profile or {}).get("level", "beginner")
        return qa_web_summarize(user_text, results, level=level)

    except Exception as e:
        dprint("handle() error:", repr(e))
        return AIMessage(content=f"[qa/error] ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”: {e!r}")